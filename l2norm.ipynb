{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 22:56:39.692751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 22:56:42.013685: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.7.0-vbhdtgc7dl4kpo4auyswsh6w3udcnf5x/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-04-27 22:56:42.013834: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.7.0-vbhdtgc7dl4kpo4auyswsh6w3udcnf5x/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-04-27 22:56:42.013848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/skoka/.venv/MLEnv3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
      "WARNING:tensorflow:From /users/skoka/.venv/MLEnv3/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_cv\n",
    "from keras_cv.models import StableDiffusion\n",
    "from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "\n",
    "\n",
    "SAVE_PATH = \"/users/skoka/Documents/Final_Paper_ML\"\n",
    "\n",
    "\n",
    "Stable_diffusion = StableDiffusion(img_height=512, img_width=512)\n",
    "decoder = Stable_diffusion.decoder\n",
    "\n",
    "diffusion_model = DiffusionModel(img_width = 512, img_height = 512, max_text_length=0)\n",
    "\n",
    "import math\n",
    "def get_timestep_embedding(timestep, batch_size, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        freqs = tf.math.exp(\n",
    "            -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
    "        )\n",
    "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
    "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
    "        embedding = tf.reshape(embedding, [1, -1])\n",
    "        return tf.repeat(embedding, batch_size, axis=0)\n",
    "\n",
    "# we are going to need to modify this code a lot \n",
    "def generate_pattern(layer_name, filter_index, size=64):\n",
    "    # Build a model that outputs the activation\n",
    "    # of the nth filter of the layer considered.\n",
    "    layer_output = diffusion_model.get_layer(layer_name).output\n",
    "    # Isolate the output \n",
    "    new_model = tf.keras.models.Model(inputs=diffusion_model.inputs, outputs=layer_output)\n",
    "    \n",
    "    # We start from a gray image with some uniform noise\n",
    "    input_img_data = np.random.random((1, size, size, 4)) * 20 + 128.\n",
    "    I = tf.Variable(input_img_data, name='image_var', dtype = 'float64')\n",
    "    #I = preprocess_input(I_start) # only process once\n",
    "    # Run gradient ascent for 40 steps\n",
    "    eta = 5\n",
    "    for i in range(100):\n",
    "        start = time.time()\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            time_embedding = get_timestep_embedding(i, 1)\n",
    "            # create a tensor with the following size of all zeros: (None, 0, 768)\n",
    "            word_embedding = tf.zeros((1, 0, 768))\n",
    "            tape.watch(I)\n",
    "            # get variable to maximize\n",
    "            model_vals = new_model((I, time_embedding, word_embedding))\n",
    "            loss = tf.reduce_mean(model_vals[:, :, :, filter_index])\n",
    "\n",
    "        # Compute the gradient of the input picture w.r.t. this loss\n",
    "        # add this operation input to maximize\n",
    "        grad_fn = tape.gradient(loss, I)\n",
    "        # Normalization trick: we normalize the gradient\n",
    "        grad_fn /= (tf.sqrt(tf.reduce_mean(tf.square(grad_fn))) + 1e-5) # mean L2 norm\n",
    "        I = I + (grad_fn * eta) # one iteration of maximizing\n",
    "        end = time.time()\n",
    "        print(\"Iteration: {}, Loss: {}, Time: {}\".format(i, loss, end-start))\n",
    "    # decode the resulting input image\n",
    "    I = decoder(I)\n",
    "    # return the numpy matrix so we can visualize \n",
    "    img = I.numpy()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers_in_order = []\n",
    "\n",
    "for layer in range(len(diffusion_model.layers)):\n",
    "    curr_layer = diffusion_model.layers[layer]\n",
    "    if len(curr_layer.get_weights()) != 0:\n",
    "        for sublayer in range(len(curr_layer.get_weights())):\n",
    "            shape = curr_layer.get_weights()[sublayer].shape\n",
    "            if len(shape) == 4 and shape[0] == 3 and shape[1] == 3:\n",
    "                cnn_layers_in_order.append((layer, sublayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0),\n",
       " (6, 2),\n",
       " (6, 8),\n",
       " (9, 2),\n",
       " (9, 8),\n",
       " (11, 0),\n",
       " (12, 2),\n",
       " (12, 8),\n",
       " (14, 2),\n",
       " (14, 8),\n",
       " (16, 0),\n",
       " (17, 2),\n",
       " (17, 8),\n",
       " (19, 2),\n",
       " (19, 8),\n",
       " (21, 0),\n",
       " (22, 2),\n",
       " (22, 8),\n",
       " (23, 2),\n",
       " (23, 8),\n",
       " (24, 2),\n",
       " (24, 8),\n",
       " (26, 2),\n",
       " (26, 8),\n",
       " (28, 2),\n",
       " (28, 8),\n",
       " (30, 2),\n",
       " (30, 8),\n",
       " (32, 2),\n",
       " (32, 8),\n",
       " (33, 0),\n",
       " (35, 2),\n",
       " (35, 8),\n",
       " (38, 2),\n",
       " (38, 8),\n",
       " (41, 2),\n",
       " (41, 8),\n",
       " (43, 0),\n",
       " (45, 2),\n",
       " (45, 8),\n",
       " (48, 2),\n",
       " (48, 8),\n",
       " (51, 2),\n",
       " (51, 8),\n",
       " (53, 0),\n",
       " (55, 2),\n",
       " (55, 8),\n",
       " (58, 2),\n",
       " (58, 8),\n",
       " (61, 2),\n",
       " (61, 8),\n",
       " (65, 0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_layers_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(layer_weights):\n",
    "    num_filters = layer_weights.shape[3]\n",
    "    norms_list = []\n",
    "    prev_filters_list = []\n",
    "    curr_filters_list = []\n",
    "    for filter in range(num_filters):\n",
    "        l2_norms = np.linalg.norm(layer_weights[:,:,:,filter], axis = (0,1))\n",
    "        # get max l2 norm\n",
    "        norms_list.append(np.max(l2_norms))\n",
    "        curr_filters_list.append(filter)\n",
    "        prev_filters_list.append(np.argmax(l2_norms))\n",
    "    norms_list = np.array(norms_list)\n",
    "    prev_filters_list = np.array(prev_filters_list)\n",
    "    index = np.argmax(norms_list)\n",
    "    return norms_list.max(), prev_filters_list[index], curr_filters_list[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7164252, 2, 235)\n",
      "(0.76154906, 73, 68)\n",
      "(0.5539933, 202, 124)\n",
      "(0.7220448, 45, 163)\n",
      "(0.55759263, 37, 11)\n",
      "(0.5239288, 235, 11)\n",
      "(3.4893906, 316, 234)\n",
      "(0.49602467, 184, 277)\n",
      "(1.1618814, 521, 301)\n",
      "(1.0276113, 301, 521)\n",
      "(0.5614104, 632, 343)\n",
      "(6.849683, 174, 36)\n",
      "(0.936096, 636, 84)\n",
      "(1.5780002, 302, 17)\n",
      "(0.7110146, 543, 959)\n",
      "(0.7115272, 1265, 225)\n",
      "(1.3082315, 283, 747)\n",
      "(1.1329187, 191, 713)\n",
      "(1.4866389, 1069, 587)\n",
      "(1.5052186, 1034, 195)\n",
      "(1.0229278, 195, 772)\n",
      "(0.9280076, 854, 1002)\n",
      "(1.3937745, 998, 1197)\n",
      "(1.6156629, 1058, 573)\n",
      "(1.2737345, 1684, 511)\n",
      "(1.5857177, 854, 270)\n",
      "(2.7754679, 734, 960)\n",
      "(1.8347987, 960, 468)\n",
      "(2.010588, 623, 37)\n",
      "(0.75336003, 510, 172)\n",
      "(0.48448202, 741, 579)\n",
      "(2.9103, 579, 785)\n",
      "(3.8729475, 1021, 477)\n",
      "(3.6221933, 901, 507)\n",
      "(1.8498381, 133, 1222)\n",
      "(10.560751, 310, 687)\n",
      "(0.82729536, 1250, 1220)\n",
      "(1.191778, 58, 361)\n",
      "(3.0310528, 802, 128)\n",
      "(1.4015703, 202, 250)\n",
      "(2.0739264, 478, 524)\n",
      "(1.3216797, 484, 266)\n",
      "(1.4742858, 526, 184)\n",
      "(0.5908569, 454, 309)\n",
      "(0.49592996, 447, 559)\n",
      "(1.8925539, 655, 77)\n",
      "(1.1917309, 182, 232)\n",
      "(1.0560087, 168, 299)\n",
      "(0.659238, 284, 14)\n",
      "(0.9728328, 140, 126)\n",
      "(0.8021537, 181, 307)\n",
      "(0.26558414, 303, 3)\n"
     ]
    }
   ],
   "source": [
    "for conv in cnn_layers_in_order:\n",
    "    layer = conv[0]\n",
    "    sublayer = conv[1]\n",
    "    layer_weights = diffusion_model.layers[layer].get_weights()[sublayer]\n",
    "    print(l2_norm(layer_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = diffusion_model.layers[cnn_layers_in_order[51][0]].get_weights()[cnn_layers_in_order[51][1]][:,:,303,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 320)\n"
     ]
    }
   ],
   "source": [
    "def get_circuit(layer, sublayer, prev_layer_filter, curr_layer_channel):\n",
    "    curr_layer_index = len(cnn_layers_in_order) - 1\n",
    "    prev_layer_index = curr_layer_index - 1\n",
    "    prev_layer_multi_channel_filter = diffusion_model.layers[cnn_layers_in_order[prev_layer_index][0]].get_weights()[cnn_layers_in_order[prev_layer_index][1]][:,:,prev_layer_filter,:]\n",
    "    \n",
    "\n",
    "\n",
    "get_circuit(cnn_layers_in_order[51][0], cnn_layers_in_order[51][1], 303, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
