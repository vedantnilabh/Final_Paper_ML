{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 16:11:44.941154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 16:12:01.014624: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.7.0-vbhdtgc7dl4kpo4auyswsh6w3udcnf5x/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-04-30 16:12:01.015168: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.7.0-vbhdtgc7dl4kpo4auyswsh6w3udcnf5x/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-04-30 16:12:01.015185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/skoka/.venv/MLEnv3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
      "WARNING:tensorflow:From /users/skoka/.venv/MLEnv3/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_cv\n",
    "from keras_cv.models import StableDiffusion\n",
    "from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "\n",
    "\n",
    "SAVE_PATH = \"/users/skoka/Documents/Final_Paper_ML\"\n",
    "\n",
    "\n",
    "Stable_diffusion = StableDiffusion(img_height=512, img_width=512)\n",
    "decoder = Stable_diffusion.decoder\n",
    "\n",
    "diffusion_model = DiffusionModel(img_width = 512, img_height = 512, max_text_length=0)\n",
    "\n",
    "import math\n",
    "def get_timestep_embedding(timestep, batch_size, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        freqs = tf.math.exp(\n",
    "            -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
    "        )\n",
    "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
    "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
    "        embedding = tf.reshape(embedding, [1, -1])\n",
    "        return tf.repeat(embedding, batch_size, axis=0)\n",
    "\n",
    "# we are going to need to modify this code a lot \n",
    "def generate_pattern(layer_index, filter_index, size=64):\n",
    "    # Build a model that outputs the activation\n",
    "    # of the nth filter of the layer considered.\n",
    "    layer_output = diffusion_model.layers[layer_index].output\n",
    "    # Isolate the output \n",
    "    new_model = tf.keras.models.Model(inputs=diffusion_model.inputs, outputs=layer_output)\n",
    "    \n",
    "    # We start from a gray image with some uniform noise\n",
    "    input_img_data = np.random.random((1, size, size, 4)) * 20 + 128.\n",
    "    I = tf.Variable(input_img_data, name='image_var', dtype = 'float64')\n",
    "    #I = preprocess_input(I_start) # only process once\n",
    "    # Run gradient ascent for 40 steps\n",
    "    eta = 5\n",
    "    for i in range(100):\n",
    "        start = time.time()\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            time_embedding = get_timestep_embedding(i, 1)\n",
    "            # create a tensor with the following size of all zeros: (None, 0, 768)\n",
    "            word_embedding = tf.zeros((1, 0, 768))\n",
    "            tape.watch(I)\n",
    "            # get variable to maximize\n",
    "            model_vals = new_model((I, time_embedding, word_embedding))\n",
    "            loss = tf.reduce_mean(model_vals[:, :, :, filter_index])\n",
    "\n",
    "        # Compute the gradient of the input picture w.r.t. this loss\n",
    "        # add this operation input to maximize\n",
    "        grad_fn = tape.gradient(loss, I)\n",
    "        # Normalization trick: we normalize the gradient\n",
    "        grad_fn /= (tf.sqrt(tf.reduce_mean(tf.square(grad_fn))) + 1e-5) # mean L2 norm\n",
    "        I = I + (grad_fn * eta) # one iteration of maximizing\n",
    "        end = time.time()\n",
    "        print(\"Iteration: {}, Loss: {}, Time: {}\".format(i, loss, end-start))\n",
    "    # decode the resulting input image\n",
    "    I = decoder(I)\n",
    "    # return the numpy matrix so we can visualize \n",
    "    img = I.numpy()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers_in_order = []\n",
    "\n",
    "for layer in range(len(diffusion_model.layers)):\n",
    "    curr_layer = diffusion_model.layers[layer]\n",
    "    if len(curr_layer.get_weights()) != 0:\n",
    "        for sublayer in range(len(curr_layer.get_weights())):\n",
    "            shape = curr_layer.get_weights()[sublayer].shape\n",
    "            if len(shape) == 4 and shape[0] == 3 and shape[1] == 3:\n",
    "                cnn_layers_in_order.append((layer, sublayer))\n",
    "\n",
    "def get_cnn_layer(index):\n",
    "    return diffusion_model.layers[cnn_layers_in_order[index][0]].get_weights()[cnn_layers_in_order[index][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0, Shape: (3, 3, 4, 320)\n",
      "Layer: 1, Shape: (3, 3, 320, 320)\n",
      "Layer: 2, Shape: (3, 3, 320, 320)\n",
      "Layer: 3, Shape: (3, 3, 320, 320)\n",
      "Layer: 4, Shape: (3, 3, 320, 320)\n",
      "Layer: 5, Shape: (3, 3, 320, 320)\n",
      "Layer: 6, Shape: (3, 3, 320, 640)\n",
      "Layer: 7, Shape: (3, 3, 640, 640)\n",
      "Layer: 8, Shape: (3, 3, 640, 640)\n",
      "Layer: 9, Shape: (3, 3, 640, 640)\n",
      "Layer: 10, Shape: (3, 3, 640, 640)\n",
      "Layer: 11, Shape: (3, 3, 640, 1280)\n",
      "Layer: 12, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 13, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 14, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 15, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 16, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 17, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 18, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 19, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 20, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 21, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 22, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 23, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 24, Shape: (3, 3, 2560, 1280)\n",
      "Layer: 25, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 26, Shape: (3, 3, 2560, 1280)\n",
      "Layer: 27, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 28, Shape: (3, 3, 2560, 1280)\n",
      "Layer: 29, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 30, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 31, Shape: (3, 3, 2560, 1280)\n",
      "Layer: 32, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 33, Shape: (3, 3, 2560, 1280)\n",
      "Layer: 34, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 35, Shape: (3, 3, 1920, 1280)\n",
      "Layer: 36, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 37, Shape: (3, 3, 1280, 1280)\n",
      "Layer: 38, Shape: (3, 3, 1920, 640)\n",
      "Layer: 39, Shape: (3, 3, 640, 640)\n",
      "Layer: 40, Shape: (3, 3, 1280, 640)\n",
      "Layer: 41, Shape: (3, 3, 640, 640)\n",
      "Layer: 42, Shape: (3, 3, 960, 640)\n",
      "Layer: 43, Shape: (3, 3, 640, 640)\n",
      "Layer: 44, Shape: (3, 3, 640, 640)\n",
      "Layer: 45, Shape: (3, 3, 960, 320)\n",
      "Layer: 46, Shape: (3, 3, 320, 320)\n",
      "Layer: 47, Shape: (3, 3, 640, 320)\n",
      "Layer: 48, Shape: (3, 3, 320, 320)\n",
      "Layer: 49, Shape: (3, 3, 640, 320)\n",
      "Layer: 50, Shape: (3, 3, 320, 320)\n",
      "Layer: 51, Shape: (3, 3, 320, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cnn_layers_in_order)):\n",
    "    print(\"Layer: {}, Shape: {}\".format(i, get_cnn_layer(i).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(layer_weights):\n",
    "    num_filters = layer_weights.shape[3]\n",
    "    norms_list = []\n",
    "    prev_filters_list = []\n",
    "    curr_filters_list = []\n",
    "    for filter in range(num_filters):\n",
    "        l2_norms = np.linalg.norm(layer_weights[:,:,:,filter], axis = (0,1))\n",
    "        # get max l2 norm\n",
    "        norms_list.append(np.max(l2_norms))\n",
    "        curr_filters_list.append(filter)\n",
    "        prev_filters_list.append(np.argmax(l2_norms))\n",
    "    norms_list = np.array(norms_list)\n",
    "    prev_filters_list = np.array(prev_filters_list)\n",
    "    index = np.argmax(norms_list)\n",
    "    return norms_list.max(), prev_filters_list[index], curr_filters_list[index]\n",
    "\n",
    "def get_l2_norms_of_single_channel_filter(layer_index, channel_index):\n",
    "    layer_weights = get_cnn_layer(layer_index)\n",
    "    l2_norms = np.linalg.norm(layer_weights[:,:,:,channel_index], axis = (0,1))\n",
    "    top_5_indices = np.argsort(l2_norms)[-5:]\n",
    "    top_5_norms = l2_norms[top_5_indices]\n",
    "    return top_5_norms, top_5_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.20631449, 0.20804347, 0.21929146, 0.2690777 , 0.2762707 ],\n",
       "       dtype=float32),\n",
       " array([238,  54,  92, 248, 311]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_l2_norms_of_single_channel_filter(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 1912 (3, 3, 1920, 640) (3, 3, 1280, 1280)\n",
      "38 1320 (3, 3, 1920, 640) (3, 3, 1280, 1280)\n",
      "38 1333 (3, 3, 1920, 640) (3, 3, 1280, 1280)\n",
      "38 1749 (3, 3, 1920, 640) (3, 3, 1280, 1280)\n",
      "[(51, 3), (50, 303), (49, 59), (48, 227), (47, 14), (46, 187), (45, 229), (44, 631), (43, 398), (42, 211), (41, 279), (40, 615), (39, 470), (38, 192), (37, 962), (36, 532), (35, 1240), (34, 82), (33, 648), (32, 1016), (31, 804), (30, 579), (29, 741), (28, 75), (27, 148), (26, 995), (25, 418), (24, 621), (23, 661), (22, 427), (21, 661), (20, 89), (19, 757), (18, 66), (17, 757), (16, 304), (15, 336), (14, 407), (13, 806), (12, 1214), (11, 305), (10, 627), (9, 386), (8, 175), (7, 412), (6, 147), (5, 7), (4, 73), (3, 270), (2, 176), (1, 100)]\n"
     ]
    }
   ],
   "source": [
    "def get_circuit(layer_index, prev_layer_filter, curr_layer_channel):\n",
    "    filters_to_visualize = []\n",
    "    filters_to_visualize.append((layer_index, curr_layer_channel))\n",
    "    layer_index -= 1\n",
    "    while layer_index > 0:\n",
    "        max_l2_norms, filter_numbers = get_l2_norms_of_single_channel_filter(layer_index, prev_layer_filter)\n",
    "        # print(get_cnn_layer(layer_index).shape, prev_layer_filter)\n",
    "        filters_to_visualize.append((layer_index, prev_layer_filter))\n",
    "        prev_layer_filter = filter_numbers[-1]\n",
    "        i = -2\n",
    "        while prev_layer_filter > get_cnn_layer(layer_index-1).shape[3]:\n",
    "            print(layer_index, prev_layer_filter, get_cnn_layer(layer_index).shape, get_cnn_layer(layer_index-1).shape)\n",
    "            prev_layer_filter = filter_numbers[i]\n",
    "            i -= 1\n",
    "        layer_index -= 1\n",
    "    print(filters_to_visualize)\n",
    "    return filters_to_visualize\n",
    "    \n",
    "circuit = get_circuit(len(cnn_layers_in_order)-1, 303, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "prev_layer_index = 0\n",
    "visualize = []\n",
    "while i < len(circuit):\n",
    "    layer_index = circuit[i][0]\n",
    "    filter_index = circuit[i][1]\n",
    "    cnn_layers_in_order[layer_index][0]\n",
    "    if cnn_layers_in_order[layer_index][0] == cnn_layers_in_order[prev_layer_index][0]:\n",
    "        i += 1\n",
    "        continue\n",
    "    prev_layer_index = layer_index\n",
    "    visualize.append((cnn_layers_in_order[layer_index][0], circuit[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(65, 3),\n",
       " (61, 303),\n",
       " (58, 227),\n",
       " (55, 187),\n",
       " (53, 631),\n",
       " (51, 398),\n",
       " (48, 279),\n",
       " (45, 470),\n",
       " (43, 962),\n",
       " (41, 532),\n",
       " (38, 82),\n",
       " (35, 1016),\n",
       " (33, 579),\n",
       " (32, 741),\n",
       " (30, 148),\n",
       " (28, 418),\n",
       " (26, 661),\n",
       " (24, 661),\n",
       " (23, 757),\n",
       " (22, 757),\n",
       " (21, 336),\n",
       " (19, 407),\n",
       " (17, 1214),\n",
       " (16, 627),\n",
       " (14, 386),\n",
       " (12, 412),\n",
       " (11, 7),\n",
       " (9, 73),\n",
       " (6, 176)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save visualize to a file called args.txt\n",
    "# it should be formatted by having each tuple on a new line with no commas or parentheses\n",
    "\n",
    "with open(SAVE_PATH + '/args.txt', 'w') as f:\n",
    "    for item in visualize:\n",
    "        f.write(str(item[0]) + ' ' + str(item[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
